---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.7
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  orphan: true
---

# Validating the GLM against scipy

```{python}
import numpy as np
import numpy.linalg as npl
import matplotlib.pyplot as plt
# Print array values to 4 decimal places
np.set_printoptions(precision=4)
import scipy.stats as sps
```

Make some random data:

```{python}
rng = np.random.default_rng()
# Make a fake regressor and data.
n = 20
x = rng.normal(10, 2, size=n)
y = rng.normal(20, 1, size=n)
plt.plot(x, y, '+')
```

Do a simple linear regression with the GLM:

$$
\newcommand{\yvec}{\vec{y}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\evec}{\vec{\varepsilon}}
\newcommand{Xmat}{\boldsymbol X}
\newcommand{\bvec}{\vec{\beta}}
\newcommand{\bhat}{\hat{\bvec}}
\newcommand{\yhat}{\hat{\yvec}}
\newcommand{\ehat}{\hat{\evec}}
\newcommand{\cvec}{\vec{c}}
\newcommand{\rank}{\textrm{rank}}
$$

$$
y_i = c + b x_i + e_i \implies \\
\yvec = \Xmat \bvec + \evec
$$

```{python}
X = np.ones((n, 2))
X[:, 1] = x
B = npl.pinv(X) @ y
B
```

```{python}
E = y - X @ B
```

Build the t statistic:

$$
\newcommand{\cvec}{\vec{c}}
$$

$$
\hat\sigma^2 = \frac{1}{n - \rank(\Xmat)} \sum e_i^2 \\
t = \frac{\cvec^T \bhat}
{\sqrt{\hat{\sigma}^2 \cvec^T (\Xmat^T \Xmat)^+ \cvec}}
$$

```{python}
# Contrast vector selects slope parameter
c = np.array([0, 1])
df = n - npl.matrix_rank(X)
sigma_2 = np.sum(E ** 2) / df
c_b_cov = c @ npl.pinv(X.T @ X) @ c
t = c @ B / np.sqrt(sigma_2 * c_b_cov)
t
```

Test the t statistic against a t distribution with `df` degrees of freedom:

```{python}
t_dist = sps.t(df=df)
p_value = 1 - t_dist.cdf(t)
# One-tailed t-test (t is positive)
p_value
```

Now do the same test with `scipy.stats.linregress`:

```{python}
res = sps.linregress(x, y, alternative='greater')
res
```

```{python}
# This is the same as the manual GLM fit
assert np.allclose(B, [res.intercept, res.slope])
# p value is always two-tailed
assert np.allclose(p_value, res.pvalue)
```

Now do the same thing with the two-sample t-test.

```{python}
X2 = np.zeros((n, 2))
X2[:10, 0] = 1
X2[10:, 1] = 1
X2
```

```{python}
B2 = npl.pinv(X2) @ y
E2 = y - X2 @ B2
c2 = np.array([1, -1])
df = n - npl.matrix_rank(X2)
df
```

```{python}
sigma_2 = np.sum(E2 ** 2) / df
c_b_cov = c2 @ npl.pinv(X2.T @ X2) @ c2
t = c2 @ B2 / np.sqrt(sigma_2 * c_b_cov)
t
```

```{python}
t_dist = sps.t(df=df)
# One-tailed p value
p_value_2 = 1 - t_dist.cdf(t)
p_value_2
```

The same thing using `scipy.stats.ttest_ind` for t test between two
independent samples:

```{python}
t_res = sps.ttest_ind(y[:10], y[10:], alternative='greater')
t_res
```

```{python}
assert np.isclose(t, t_res.statistic)
assert np.isclose(p_value_2, t_res.pvalue)
```
